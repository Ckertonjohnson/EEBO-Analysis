import pandas as pd
import os
import json
import re
import logging
import gc
from collections import defaultdict
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from google.colab import drive

# Configure logging
logging.basicConfig(filename='/content/drive/MyDrive/EEBO/EEBO_Analysis/word_count_logs.log', level=logging.INFO, 
                    format='%(asctime)s %(levelname)s:%(message)s')

# Define paths
input_directory = '/content/drive/MyDrive/EEBO/EEBO_Analysis/cleaned_metadata_csv/metadata_csv'
progress_tracker_path = '/content/drive/MyDrive/EEBO/EEBO_Analysis/Stimming words analysis/stimming_word_count_progress_tracker.json'
output_file_path = '/content/drive/MyDrive/EEBO/EEBO_Analysis/Stimming words analysis/stimming_word_count_results.json'

# Create output directory if it doesn't exist
os.makedirs(os.path.dirname(output_file_path), exist_ok=True)

# Function to load progress
def load_progress(progress_tracker_path):
    try:
        with open(progress_tracker_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"last_processed_file": None, "last_processed_row": 0}

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_count_results)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Function to save progress
def save_progress(progress, progress_tracker_path):
    with open(progress_tracker_path, 'w') as f:
        json.dump(progress, f)

# Function to perform word count
def count_words(text, word_variants):
    word_count = defaultdict(int)
    for word, variants in word_variants.items():
        for variant in variants:
            word_count[word] += len(re.findall(r'\b' + re.escape(variant) + r'\b', text, re.IGNORECASE))
    return word_count

# Function to save word count results
def save_word_count_results(word_count_results, output_file_path):
    with open(output_file_path, 'w') as f:
        json.dump(word_count_results, f)

# Iterate through all CSV files in the input directory
for root, dirs, files in os.walk(input_directory):
    for file_name in sorted(files):
        if file_name.endswith('.csv'):
            file_path = os.path.join(root, file_name)
            
            # Skip files that have already been processed
            if progress_tracker["last_processed_file"] and file_name <= progress_tracker["last_processed_file"]:
                continue
            
            try:
                df = pd.read_csv(file_path)
                
                # Debugging: print the number of rows in the current CSV file
                print(f"Processing file {file_name} with {df.shape[0]} rows")
                
                # Resume from the last processed row if this file was interrupted
                start_row = progress_tracker["last_processed_row"] if file_name == progress_tracker["last_processed_file"] else 0

                # Perform word counting on the 'ProcessedText' column
                for index in range(start_row, len(df)):
                    row = df.iloc[index]
                    text = row['ProcessedText']
                    word_counts = count_words(text, stimming_word_variants)
                    
                    # Process only if the word count is greater than zero
                    if any(word_counts.values()):
                        for word, count in word_counts.items():
                            word_count_results[word] += count
                    
                    # Update progress tracker
                    progress_tracker = {"last_processed_file": file_name, "last_processed_row": index + 1}
                    
                    # Clear memory and print progress
                    if (index + 1) % 50 == 0:
                        gc.collect()
                        print(f"Processed {index + 1} rows in {file_name}")
                        save_progress(progress_tracker, progress_tracker_path)

                # Save intermediate results after processing each file
                save_word_count_results(word_count_results, output_file_path)
                print(f"Intermediate word count results saved after processing {file_name}")
                
                # Save progress after processing each file
                save_progress(progress_tracker, progress_tracker_path)

            except Exception as e:
                logging.error(f"Error processing file {file_path}: {e}")
                continue

# Save the final word count results
with open(output_file_path, 'w') as f:
    json.dump(word_count_results, f)
print(f"Word count results saved to {output_file_path}")

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_count_results)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
